{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f6d567",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Comments & Instructions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f44282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a example/template for the `read_data.py` script, that loads the\n",
    "# data from a specific paper and reformats it into the standard format used by\n",
    "# SPAG. To use this, copy the following function into a testing script or\n",
    "# Jupyter notebook, and modify the function name and contents as needed.\n",
    "\n",
    "\n",
    "# Specifically, change all instances of...\n",
    "\n",
    "# - `authorYYYYx` to the first author's last name (maybe with their first \n",
    "# initial of there is a duplicate last name) and year of the paper. When there\n",
    "# are instances of an author releasing more than one paper in a year, append a\n",
    "# letter to the year in `x` position.\n",
    "#   (e.g., `Ji et al. 2016a` --> `ji2016a`, if there is also a \n",
    "#       `Ji et al. 2016b`)\n",
    "#   (e.g., `Hansen et al. 2018` --> `hansent2018c`, if there are multiple\n",
    "#       authors with the same last name and this was T. Hansen's 3rd paper in \n",
    "#       2018)\n",
    "\n",
    "# - the path to the data files to point to the correct location of the data \n",
    "#   tables.\n",
    "#   (e.g., `data_dir + \"abundance_tables/authorYYYYx/table1.csv\"`)\n",
    "\n",
    "# - The `Reference` and `Ref` columns to match the citation for the paper.\n",
    "#   --> `Reference` column should be in the form 'Author+Year'.\n",
    "#       (e.g., 'Ji+2016a', 'HansenT+2018c')\n",
    "#   --> `Ref` column should shortened form of the reference, typically the \n",
    "#       first 3 letters of the author's last name (all uppercase) and the last\n",
    "#       two digits of the year, with a letter appended if there are multiple\n",
    "#       papers by the same author(s) in that year. (e.g.'JI16a', 'HANt18c')\n",
    "\n",
    "# - The `Loc` column to match the type of object the stars are in. Use:\n",
    "#   --> 'HA' for halo stars\n",
    "#   --> 'BU' for bulge stars\n",
    "#   --> 'DS' for disk stars\n",
    "#   --> 'DW' for dwarf galaxy stars\n",
    "#   --> 'UF' for ultra-faint dwarf galaxy stars\n",
    "#   --> 'GC' for globular cluster stars\n",
    "\n",
    "# - The `System` column to match the name of the system the stars are in, if \n",
    "#   applicable and not already in the data table.\n",
    "\n",
    "\n",
    "# Beyond this, confirm that the datafiles are reading in correctly, and that\n",
    "# the columns are being filled in properly. You may need to modify how the data \n",
    "# is read in or the structure of the datafile themselves, depending on the \n",
    "# format of the data tables.\n",
    "\n",
    "# Things to be wary of...\n",
    "\n",
    "# - Some papers use different formats for the species names, such as \"Fe I\" vs\n",
    "#   \"FeI\" vs \"Fe1\". The `ion_to_species()` function should be able to handle \n",
    "#   most of these, but you may need to modify the datafile if there are any \n",
    "#   issues, or contact A. Yelland.\n",
    "\n",
    "# - Some papers may not provide all the necessary columns, such as separate\n",
    "#   upper/lower limit flag column. You may need to modify the datafile to add\n",
    "#   the flag columns.\n",
    "\n",
    "# - Additionally, different authors at different times may use different solar\n",
    "#   abundance scales. The `get_solar()` function can be used to retrieve the\n",
    "#   solar abundances of the most commonly used scales, but if one is not\n",
    "#   available, please let A. Yelland know.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153eca",
   "metadata": {},
   "source": [
    "---\n",
    "## Package Imports & Directories\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65303612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.sgr]\n",
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.orphan]\n",
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.magellanic_stream]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import (division, print_function, absolute_import,\n",
    "                        unicode_literals)\n",
    "import  sys, os, glob, time, IPython\n",
    "\n",
    "import astropy.constants as const\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Table\n",
    "# from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "\n",
    "# from PyAstronomy import pyasl\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"colorblind\")\n",
    "colors = sns.color_palette(\"colorblind\", 20)\n",
    "\n",
    "# from smh import Session\n",
    "\n",
    "from spag.read_data import *\n",
    "from spag.convert import *\n",
    "from spag.utils import *\n",
    "# from spag.calculate import *\n",
    "# import spag.read_data as rd\n",
    "import spag.coordinates as coord\n",
    "\n",
    "# import alexmods.read_data as rd\n",
    "\n",
    "script_dir = \"/\".join(IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[:-1]) + \"/\"\n",
    "# script_dir = os.path.dirname(os.path.realpath(__file__))+\"/\"\n",
    "data_dir = '/Users/ayelland/Research/metal-poor-stars/spag/spag/data/'\n",
    "plotting_dir = script_dir+\"plots/\"\n",
    "if not os.path.exists(plotting_dir):\n",
    "    os.makedirs(plotting_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb48c53",
   "metadata": {},
   "source": [
    "---\n",
    "## Template Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b595691",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ayelland/Research/metal-poor-stars/spag/spag/data/abundance_tables/authorYYYYx/table1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Fe/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul[Fe/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[FeII/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul[FeII/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m authorYYYYx_df\n\u001b[0;32m--> 112\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_authorYYYYx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m display(df)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mload_authorYYYYx\u001b[0;34m(io)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mLoad the Author et al. YYYYx data for the <grouping or classification of stars>.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mTable 3 - Abundance Table\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m## Read in the data tables\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m obs_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabundance_tables/authorYYYYx/table1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNaN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mN/A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn/a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m param_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabundance_tables/authorYYYYx/table3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m, na_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/a\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m abund_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabundance_tables/authorYYYYx/table4.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m, na_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/a\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ayelland/Research/metal-poor-stars/spag/spag/data/abundance_tables/authorYYYYx/table1.csv'"
     ]
    }
   ],
   "source": [
    "def load_authorYYYYx(io=None):\n",
    "    \"\"\"\n",
    "    Load the Author et al. YYYYx data for the <grouping or classification of stars>.\n",
    "\n",
    "    Table 1 - Observations\n",
    "    Table 2 - Stellar Parameters\n",
    "    Table 3 - Abundance Table\n",
    "    \"\"\"\n",
    "\n",
    "    ## Read in the data tables\n",
    "    obs_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table1.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    param_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table3.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    abund_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table4.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "\n",
    "    ## Make the new column names\n",
    "    species = []\n",
    "    for ion in abund_df[\"Species\"].unique():\n",
    "        species_i = ion_to_species(ion)\n",
    "        elem_i = ion_to_element(ion)\n",
    "        if species_i not in species:\n",
    "            species.append(species_i)\n",
    "\n",
    "    epscols = [make_epscol(s) for s in species]\n",
    "    ulcols = [make_ulcol(s) for s in species]\n",
    "    XHcols = [make_XHcol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXHcols = ['ul' + col for col in XHcols]\n",
    "    XFecols = [make_XFecol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXFecols = ['ul' + col for col in XFecols]\n",
    "    errcols = [make_errcol(s) for s in species]\n",
    "\n",
    "    ## New dataframe with proper columns\n",
    "    authorYYYYx_df = pd.DataFrame(\n",
    "                    columns=['I/O','Name','Simbad_Identifier','Reference','Ref','Loc','System','RA_hms','RA_deg','DEC_dms','DEC_deg',\n",
    "                    'Teff','logg','Fe/H','Vmic'] + epscols + ulcols + XHcols + ulXHcols + XFecols \n",
    "                    + ulXFecols + errcols)\n",
    "    for i, name in enumerate(abund_df['Name'].unique()):\n",
    "        authorYYYYx_df.loc[i,'Name'] = name\n",
    "        authorYYYYx_df.loc[i,'Simbad_Identifier'] = obs_df.loc[obs_df['Name'] == name, 'Simbad_Identifier'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Reference'] = 'Author+YYYYx'\n",
    "        authorYYYYx_df.loc[i,'Ref'] = 'AUTYYx'\n",
    "        authorYYYYx_df.loc[i,'I/O'] = 1\n",
    "        authorYYYYx_df.loc[i,'Loc'] = 'UF' # [HA, BU, DS, DW, UF, GC]\n",
    "        authorYYYYx_df.loc[i,'System'] = obs_df.loc[obs_df['Name'] == name, 'System'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_hms'] = obs_df.loc[obs_df['Name'] == name, 'RA_hms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_deg'] = coord.ra_hms_to_deg(authorYYYYx_df.loc[i,'RA_hms'], precision=6)\n",
    "        authorYYYYx_df.loc[i,'DEC_dms'] = obs_df.loc[obs_df['Name'] == name, 'DEC_dms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'DEC_deg'] = coord.dec_dms_to_deg(authorYYYYx_df.loc[i,'DEC_dms'], precision=2)\n",
    "        authorYYYYx_df.loc[i,'Teff'] = param_df.loc[param_df['Name'] == name, 'Teff'].values[0]\n",
    "        authorYYYYx_df.loc[i,'logg'] = param_df.loc[param_df['Name'] == name, 'logg'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Fe/H'] = param_df.loc[param_df['Name'] == name, 'Fe/H'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Vmic'] = param_df.loc[param_df['Name'] == name, 'Vmic'].values[0]\n",
    "\n",
    "        ## Fill in data\n",
    "        star_df = abund_df[abund_df['Name'] == name]\n",
    "        for j, row in star_df.iterrows():\n",
    "            ion = row[\"Species\"]\n",
    "            species_i = ion_to_species(ion)\n",
    "            elem_i = ion_to_element(ion)\n",
    "\n",
    "            logepsX_sun_a09 = get_solar(elem_i, version='asplund2009')[0]\n",
    "            logepsFe_a09 = star_df.loc[star_df['Species'] == 'Fe I', 'logepsX'].values[0]\n",
    "            feh_a09 = logepsFe_a09 - get_solar('Fe', version='asplund2009')[0]\n",
    "\n",
    "            ## Assign epsX values\n",
    "            col = make_epscol(species_i)\n",
    "            if col in epscols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.isna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign ulX values\n",
    "            col = make_ulcol(species_i)\n",
    "            if col in ulcols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.notna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign [X/H] and ul[X/H]values\n",
    "            col = make_XHcol(species_i).replace(\" \", \"\")\n",
    "            if col in XHcols:\n",
    "                if pd.isna(row[\"l_[X/H]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                if 'e_[X/H]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/H]\"]\n",
    "\n",
    "            ## Assign [X/Fe] values\n",
    "            col = make_XFecol(species_i).replace(\" \", \"\")\n",
    "            if col in XFecols:\n",
    "                if pd.isna(row[\"l_[X/Fe]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                if 'e_[X/Fe]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/Fe]\"]\n",
    "\n",
    "            ## Assign error values\n",
    "            col = make_errcol(species_i)\n",
    "            if col in errcols:\n",
    "                e_logepsX = row.get(\"e_logepsX\", np.nan)\n",
    "                if pd.notna(e_logepsX):\n",
    "                    authorYYYYx_df.loc[i, col] = e_logepsX\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "\n",
    "    ## Drop the Fe/Fe columns\n",
    "    authorYYYYx_df.drop(columns=['[Fe/Fe]','ul[Fe/Fe]','[FeII/Fe]','ul[FeII/Fe]'], inplace=True, errors='ignore')\n",
    "\n",
    "    return authorYYYYx_df\n",
    "\n",
    "df = load_authorYYYYx()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceecde7",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can copy/paste the above function, and modify it for your specific reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20663730",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Conversion Scripts for Different Datafiles & Numbers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ce3c2",
   "metadata": {},
   "source": [
    "### Abundance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd4e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logeps(Fe):  1.88\n",
      "logeps(Sr): <-3.90\n",
      "logeps(Ba):  -3.50\n",
      "[Fe/H]:      -5.62\n",
      "[Sr/H]:     <-6.77\n",
      "[Ba/H]:      -5.68\n",
      "[Sr/Fe]:    <-1.15\n",
      "[Ba/Fe]:     -0.06\n"
     ]
    }
   ],
   "source": [
    "## Quick Abundance Conversions\n",
    "\n",
    "### Solar Abundances\n",
    "epsfe_sun = rd.get_solar('Fe')[0]\n",
    "epsba_sun = rd.get_solar('Ba')[0]\n",
    "epssr_sun = rd.get_solar('Sr')[0]\n",
    "\n",
    "### From Anna's notes of a presentation\n",
    "srfe = -1.15\n",
    "epsba = -3.5\n",
    "feh = -5.62\n",
    "\n",
    "### Calculated log(eps) values\n",
    "srh = srfe + feh\n",
    "epssr = epssr_sun + srh\n",
    "epsfe = epsfe_sun + feh\n",
    "\n",
    "feh_new = epsfe - epsfe_sun\n",
    "srh_new = epssr - epssr_sun\n",
    "bah_new = epsba - epsba_sun\n",
    "\n",
    "srfe_new = srh_new - feh_new\n",
    "bafe_new = bah_new - feh_new\n",
    "\n",
    "print(f\"logeps(Fe):  {epsfe:.2f}\")\n",
    "print(f\"logeps(Sr): <{epssr:.2f}\")\n",
    "print(f\"logeps(Ba):  {epsba:.2f}\")\n",
    "print(f\"[Fe/H]:      {feh_new:.2f}\")\n",
    "print(f\"[Sr/H]:     <{srh_new:.2f}\")\n",
    "print(f\"[Ba/H]:      {bah_new:.2f}\")\n",
    "print(f\"[Sr/Fe]:    <{srfe_new:.2f}\")\n",
    "print(f\"[Ba/Fe]:     {bafe_new:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167a4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XH_from_eps(0.73, 'Sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac193c",
   "metadata": {},
   "source": [
    "### CDS $\\rightarrow$ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert all CDS Standard Formatted Tables to CSVs for Stellar Abundances\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system =  'cayrel2004'\n",
    "    table_numbers = [2,3,8]\n",
    "    for n in table_numbers:\n",
    "\n",
    "        base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data_templates/cayrel2004/\"\n",
    "        table_path = base_path + f\"table{n}.dat\"\n",
    "        readme_path = base_path + f\"ReadMe\"\n",
    "        csv_path = base_path + f\"table{n}.csv\"\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            table_data = ascii.read(table_path, format='cds', readme=readme_path)\n",
    "                    # guess=False,\n",
    "                    # fast_reader=False\n",
    "                    \n",
    "            table_data = table_data.to_pandas()\n",
    "            table_data.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Table {n} or ReadMe file does not exist for {system}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted table1 to CSV.\n",
      "❌ Failed to read table2: Column V(MW) failed to convert: invalid literal for int() with base 10: 'nan'\n",
      "✅ Converted table3 to CSV.\n",
      "✅ Converted table4 to CSV.\n",
      "✅ Converted table5 to CSV.\n"
     ]
    }
   ],
   "source": [
    "## Convert CDS Standard Formatted Tables to CSVs for Galaxy Properties\n",
    "\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system = 'mcconnachie2012'\n",
    "    table_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data/galaxy_properties/{system}/\"\n",
    "    readme_path = os.path.join(base_path, \"ReadMe\")\n",
    "\n",
    "    for n in table_numbers:\n",
    "        table_path = os.path.join(base_path, f\"table{n}.dat\")\n",
    "        csv_path = os.path.join(base_path, f\"table{n}.csv\")\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            try:\n",
    "                table_data = ascii.read(\n",
    "                    table_path,\n",
    "                    format='cds',\n",
    "                    readme=readme_path,\n",
    "                    guess=False,\n",
    "                    fast_reader=False,\n",
    "                    fill_values=[('-', 'nan'), ('--', 'nan'), ('...', 'nan')]\n",
    "                )\n",
    "                table_data = table_data.to_pandas()\n",
    "                table_data.to_csv(csv_path, index=False)\n",
    "                print(f\"✅ Converted table{n} to CSV.\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to read table{n}: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Missing table{n}.dat or ReadMe file for {system}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d0f7b",
   "metadata": {},
   "source": [
    "### HMS/DMS $\\leftrightarrow$ Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f17dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Coordinates:\n",
      "\n",
      "(16:10:32.30 , -08:15:38.50)\n",
      "(15:05:02.13 , -13:44:20.23)\n",
      "(18:09:43.61 , -40:52:07.99)\n"
     ]
    }
   ],
   "source": [
    "## Convert coordinates between degrees and hms/dms format\n",
    "\n",
    "data = '''\n",
    "242.63458901020093, -8.260693273003724 \n",
    "226.25888112673744, -13.738953843436517\n",
    "272.43169186461097, -40.86888561990257 \n",
    "'''\n",
    "\n",
    "def convert_deg_to_hmsdms(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_deg, dec_deg = map(float, line.split(','))\n",
    "        ra_hms = coord.ra_deg_to_hms(ra_deg, precision=2)\n",
    "        dec_dms = coord.dec_deg_to_dms(dec_deg, precision=2)\n",
    "        coords.append((ra_hms, dec_dms))\n",
    "        # print(f\"{ra_hms}, {dec_dms}\")\n",
    "    return coords\n",
    "\n",
    "def convert_hmsdms_to_deg(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_hms, dec_hms = map(str, line.split(','))\n",
    "        ra_deg = coord.ra_hms_to_deg(ra_hms, precision=6)\n",
    "        dec_deg = coord.dec_dms_to_deg(dec_hms, precision=6)\n",
    "        coords.append((ra_deg, dec_deg))\n",
    "        # print(f\"{ra_deg}, {dec_deg}\")\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Convert the coordinates\n",
    "try:\n",
    "    txt = convert_deg_to_hmsdms(data)\n",
    "except:\n",
    "    try:\n",
    "        txt = convert_hmsdms_to_deg(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting coordinates: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"Converted Coordinates:\\n\")\n",
    "for c in txt:\n",
    "    print(f'({c[0]} , {c[1]})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
