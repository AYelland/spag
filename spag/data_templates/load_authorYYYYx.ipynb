{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f6d567",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Comments & Instructions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f44282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a example/template for the `read_data.py` script, that loads the\n",
    "# data from a specific paper and reformats it into the standard format used by\n",
    "# SPAG. To use this, copy the following function into a testing script or\n",
    "# Jupyter notebook, and modify the function name and contents as needed.\n",
    "\n",
    "\n",
    "# Specifically, change all instances of...\n",
    "\n",
    "# - `authorYYYYx` to the first author's last name (maybe with their first \n",
    "# initial of there is a duplicate last name) and year of the paper. When there\n",
    "# are instances of an author releasing more than one paper in a year, append a\n",
    "# letter to the year in `x` position.\n",
    "#   (e.g., `Ji et al. 2016a` --> `ji2016a`, if there is also a \n",
    "#       `Ji et al. 2016b`)\n",
    "#   (e.g., `Hansen et al. 2018` --> `hansent2018c`, if there are multiple\n",
    "#       authors with the same last name and this was T. Hansen's 3rd paper in \n",
    "#       2018)\n",
    "\n",
    "# - the path to the data files to point to the correct location of the data \n",
    "#   tables.\n",
    "#   (e.g., `data_dir + \"abundance_tables/authorYYYYx/table1.csv\"`)\n",
    "\n",
    "# - The `Reference` and `Ref` columns to match the citation for the paper.\n",
    "#   --> `Reference` column should be in the form 'Author+Year'.\n",
    "#       (e.g., 'Ji+2016a', 'HansenT+2018c')\n",
    "#   --> `Ref` column should shortened form of the reference, typically the \n",
    "#       first 3 letters of the author's last name (all uppercase) and the last\n",
    "#       two digits of the year, with a letter appended if there are multiple\n",
    "#       papers by the same author(s) in that year. (e.g.'JI16a', 'HANt18c')\n",
    "\n",
    "# - The `Loc` column to match the type of object the stars are in. Use:\n",
    "#   --> 'HA' for halo stars\n",
    "#   --> 'BU' for bulge stars\n",
    "#   --> 'DS' for disk stars\n",
    "#   --> 'DW' for dwarf galaxy stars\n",
    "#   --> 'UF' for ultra-faint dwarf galaxy stars\n",
    "#   --> 'GC' for globular cluster stars\n",
    "\n",
    "# - The `System` column to match the name of the system the stars are in, if \n",
    "#   applicable and not already in the data table.\n",
    "\n",
    "\n",
    "# Beyond this, confirm that the datafiles are reading in correctly, and that\n",
    "# the columns are being filled in properly. You may need to modify how the data \n",
    "# is read in or the structure of the datafile themselves, depending on the \n",
    "# format of the data tables.\n",
    "\n",
    "# Things to be wary of...\n",
    "\n",
    "# - Some papers use different formats for the species names, such as \"Fe I\" vs\n",
    "#   \"FeI\" vs \"Fe1\". The `ion_to_species()` function should be able to handle \n",
    "#   most of these, but you may need to modify the datafile if there are any \n",
    "#   issues, or contact A. Yelland.\n",
    "\n",
    "# - Some papers may not provide all the necessary columns, such as separate\n",
    "#   upper/lower limit flag column. You may need to modify the datafile to add\n",
    "#   the flag columns.\n",
    "\n",
    "# - Additionally, different authors at different times may use different solar\n",
    "#   abundance scales. The `get_solar()` function can be used to retrieve the\n",
    "#   solar abundances of the most commonly used scales, but if one is not\n",
    "#   available, please let A. Yelland know.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153eca",
   "metadata": {},
   "source": [
    "---\n",
    "## Package Imports & Directories\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65303612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import (division, print_function, absolute_import,\n",
    "                        unicode_literals)\n",
    "import  sys, os, glob, time, IPython\n",
    "\n",
    "import astropy.constants as const\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Table\n",
    "# from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "\n",
    "# from PyAstronomy import pyasl\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"colorblind\")\n",
    "colors = sns.color_palette(\"colorblind\", 20)\n",
    "\n",
    "# from smh import Session\n",
    "\n",
    "from spag.read_data import *\n",
    "from spag.convert import *\n",
    "from spag.utils import *\n",
    "# from spag.calculate import *\n",
    "# import spag.read_data as rd\n",
    "import spag.coordinates as scoord\n",
    "\n",
    "# import alexmods.read_data as rd\n",
    "\n",
    "script_dir = \"/\".join(IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[:-1]) + \"/\"\n",
    "# script_dir = os.path.dirname(os.path.realpath(__file__))+\"/\"\n",
    "data_dir = '/Users/ayelland/Research/metal-poor-stars/spag/spag/data_templates/'\n",
    "plotting_dir = script_dir+\"plots/\"\n",
    "if not os.path.exists(plotting_dir):\n",
    "    os.makedirs(plotting_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb48c53",
   "metadata": {},
   "source": [
    "---\n",
    "## Template Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b595691",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Simbad_Identifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Simbad_Identifier'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Fe/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul[Fe/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[FeII/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul[FeII/Fe]\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m authorYYYYx_df\n\u001b[0;32m--> 112\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_authorYYYYx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m display(df)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mload_authorYYYYx\u001b[0;34m(io)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(abund_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()):\n\u001b[1;32m     37\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 38\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimbad_Identifier\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mobs_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimbad_Identifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReference\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor+YYYYx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     authorYYYYx_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRef\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUTYYx\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexing.py:1280\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1279\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexing.py:1000\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   1006\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexing.py:1293\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/generic.py:4082\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_level:\n\u001b[0;32m-> 4082\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   4083\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.anaconda3/envs/spag/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Simbad_Identifier'"
     ]
    }
   ],
   "source": [
    "def load_authorYYYYx(io=None):\n",
    "    \"\"\"\n",
    "    Load the Author et al. YYYYx data for the <grouping or classification of stars>.\n",
    "\n",
    "    Table 1 - Observations\n",
    "    Table 2 - Stellar Parameters\n",
    "    Table 3 - Abundance Table\n",
    "    \"\"\"\n",
    "\n",
    "    ## Read in the data tables\n",
    "    obs_df = pd.read_csv(data_dir + \"authorYYYYx/table1.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    param_df = pd.read_csv(data_dir + \"authorYYYYx/table3.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    abund_df = pd.read_csv(data_dir + \"authorYYYYx/table4.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "\n",
    "    ## Make the new column names\n",
    "    species = []\n",
    "    for ion in abund_df[\"Species\"].unique():\n",
    "        species_i = ion_to_species(ion)\n",
    "        elem_i = ion_to_element(ion)\n",
    "        if species_i not in species:\n",
    "            species.append(species_i)\n",
    "\n",
    "    epscols = [make_epscol(s) for s in species]\n",
    "    ulcols = [make_ulcol(s) for s in species]\n",
    "    XHcols = [make_XHcol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXHcols = ['ul' + col for col in XHcols]\n",
    "    XFecols = [make_XFecol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXFecols = ['ul' + col for col in XFecols]\n",
    "    errcols = [make_errcol(s) for s in species]\n",
    "\n",
    "    ## New dataframe with proper columns\n",
    "    authorYYYYx_df = pd.DataFrame(\n",
    "                    columns=['I/O','Name','Simbad_Identifier','Reference','Ref','Loc','System','RA_hms','RA_deg','DEC_dms','DEC_deg',\n",
    "                    'Teff','logg','Fe/H','Vmic'] + epscols + ulcols + XHcols + ulXHcols + XFecols \n",
    "                    + ulXFecols + errcols)\n",
    "    for i, name in enumerate(abund_df['Name'].unique()):\n",
    "        authorYYYYx_df.loc[i,'Name'] = name\n",
    "        authorYYYYx_df.loc[i,'Simbad_Identifier'] = obs_df.loc[obs_df['Name'] == name, 'Simbad_Identifier'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Reference'] = 'Author+YYYYx'\n",
    "        authorYYYYx_df.loc[i,'Ref'] = 'AUTYYx'\n",
    "        authorYYYYx_df.loc[i,'I/O'] = 1\n",
    "        authorYYYYx_df.loc[i,'Loc'] = 'UF' # [HA, BU, DS, DW, UF, GC]\n",
    "        authorYYYYx_df.loc[i,'System'] = obs_df.loc[obs_df['Name'] == name, 'System'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_hms'] = obs_df.loc[obs_df['Name'] == name, 'RA_hms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_deg'] = scoord.ra_hms_to_deg(authorYYYYx_df.loc[i,'RA_hms'], precision=6)\n",
    "        authorYYYYx_df.loc[i,'DEC_dms'] = obs_df.loc[obs_df['Name'] == name, 'DEC_dms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'DEC_deg'] = scoord.dec_dms_to_deg(authorYYYYx_df.loc[i,'DEC_dms'], precision=2)\n",
    "        authorYYYYx_df.loc[i,'Teff'] = param_df.loc[param_df['Name'] == name, 'Teff'].values[0]\n",
    "        authorYYYYx_df.loc[i,'logg'] = param_df.loc[param_df['Name'] == name, 'logg'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Fe/H'] = param_df.loc[param_df['Name'] == name, 'Fe/H'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Vmic'] = param_df.loc[param_df['Name'] == name, 'Vmic'].values[0]\n",
    "\n",
    "        ## Fill in data\n",
    "        star_df = abund_df[abund_df['Name'] == name]\n",
    "        for j, row in star_df.iterrows():\n",
    "            ion = row[\"Species\"]\n",
    "            species_i = ion_to_species(ion)\n",
    "            elem_i = ion_to_element(ion)\n",
    "\n",
    "            logepsX_sun_a09 = get_solar(elem_i, version='asplund2009')[0]\n",
    "            logepsFe_a09 = star_df.loc[star_df['Species'] == 'Fe I', 'logepsX'].values[0]\n",
    "            feh_a09 = logepsFe_a09 - get_solar('Fe', version='asplund2009')[0]\n",
    "\n",
    "            ## Assign epsX values\n",
    "            col = make_epscol(species_i)\n",
    "            if col in epscols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.isna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign ulX values\n",
    "            col = make_ulcol(species_i)\n",
    "            if col in ulcols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.notna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign [X/H] and ul[X/H]values\n",
    "            col = make_XHcol(species_i).replace(\" \", \"\")\n",
    "            if col in XHcols:\n",
    "                if pd.isna(row[\"l_[X/H]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                if 'e_[X/H]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/H]\"]\n",
    "\n",
    "            ## Assign [X/Fe] values\n",
    "            col = make_XFecol(species_i).replace(\" \", \"\")\n",
    "            if col in XFecols:\n",
    "                if pd.isna(row[\"l_[X/Fe]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                if 'e_[X/Fe]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/Fe]\"]\n",
    "\n",
    "            ## Assign error values\n",
    "            col = make_errcol(species_i)\n",
    "            if col in errcols:\n",
    "                e_logepsX = row.get(\"e_logepsX\", np.nan)\n",
    "                if pd.notna(e_logepsX):\n",
    "                    authorYYYYx_df.loc[i, col] = e_logepsX\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "\n",
    "    ## Drop the Fe/Fe columns\n",
    "    authorYYYYx_df.drop(columns=['[Fe/Fe]','ul[Fe/Fe]','[FeII/Fe]','ul[FeII/Fe]'], inplace=True, errors='ignore')\n",
    "\n",
    "    return authorYYYYx_df\n",
    "\n",
    "df = load_authorYYYYx()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceecde7",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can copy/paste the above function, and modify it for your specific reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20663730",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Conversion Scripts for Different Datafiles & Numbers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ce3c2",
   "metadata": {},
   "source": [
    "### Abundance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd4e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logeps(Fe):  1.88\n",
      "logeps(Sr): <-3.90\n",
      "logeps(Ba):  -3.50\n",
      "[Fe/H]:      -5.62\n",
      "[Sr/H]:     <-6.77\n",
      "[Ba/H]:      -5.68\n",
      "[Sr/Fe]:    <-1.15\n",
      "[Ba/Fe]:     -0.06\n"
     ]
    }
   ],
   "source": [
    "## Quick Abundance Conversions\n",
    "\n",
    "### Solar Abundances\n",
    "epsfe_sun = rd.get_solar('Fe')[0]\n",
    "epsba_sun = rd.get_solar('Ba')[0]\n",
    "epssr_sun = rd.get_solar('Sr')[0]\n",
    "\n",
    "### From Anna's notes of a presentation\n",
    "srfe = -1.15\n",
    "epsba = -3.5\n",
    "feh = -5.62\n",
    "\n",
    "### Calculated log(eps) values\n",
    "srh = srfe + feh\n",
    "epssr = epssr_sun + srh\n",
    "epsfe = epsfe_sun + feh\n",
    "\n",
    "feh_new = epsfe - epsfe_sun\n",
    "srh_new = epssr - epssr_sun\n",
    "bah_new = epsba - epsba_sun\n",
    "\n",
    "srfe_new = srh_new - feh_new\n",
    "bafe_new = bah_new - feh_new\n",
    "\n",
    "print(f\"logeps(Fe):  {epsfe:.2f}\")\n",
    "print(f\"logeps(Sr): <{epssr:.2f}\")\n",
    "print(f\"logeps(Ba):  {epsba:.2f}\")\n",
    "print(f\"[Fe/H]:      {feh_new:.2f}\")\n",
    "print(f\"[Sr/H]:     <{srh_new:.2f}\")\n",
    "print(f\"[Ba/H]:      {bah_new:.2f}\")\n",
    "print(f\"[Sr/Fe]:    <{srfe_new:.2f}\")\n",
    "print(f\"[Ba/Fe]:     {bafe_new:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167a4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XH_from_eps(0.73, 'Sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac193c",
   "metadata": {},
   "source": [
    "### CDS $\\rightarrow$ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert all CDS Standard Formatted Tables to CSVs for Stellar Abundances\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system =  'cayrel2004'\n",
    "    table_numbers = [2,3,8]\n",
    "    for n in table_numbers:\n",
    "\n",
    "        base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data_templates/cayrel2004/\"\n",
    "        table_path = base_path + f\"table{n}.dat\"\n",
    "        readme_path = base_path + f\"ReadMe\"\n",
    "        csv_path = base_path + f\"table{n}.csv\"\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            table_data = ascii.read(table_path, format='cds', readme=readme_path)\n",
    "                    # guess=False,\n",
    "                    # fast_reader=False\n",
    "                    \n",
    "            table_data = table_data.to_pandas()\n",
    "            table_data.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Table {n} or ReadMe file does not exist for {system}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted table1 to CSV.\n",
      "❌ Failed to read table2: Column V(MW) failed to convert: invalid literal for int() with base 10: 'nan'\n",
      "✅ Converted table3 to CSV.\n",
      "✅ Converted table4 to CSV.\n",
      "✅ Converted table5 to CSV.\n"
     ]
    }
   ],
   "source": [
    "## Convert CDS Standard Formatted Tables to CSVs for Galaxy Properties\n",
    "\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system = 'mcconnachie2012'\n",
    "    table_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data/galaxy_properties/{system}/\"\n",
    "    readme_path = os.path.join(base_path, \"ReadMe\")\n",
    "\n",
    "    for n in table_numbers:\n",
    "        table_path = os.path.join(base_path, f\"table{n}.dat\")\n",
    "        csv_path = os.path.join(base_path, f\"table{n}.csv\")\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            try:\n",
    "                table_data = ascii.read(\n",
    "                    table_path,\n",
    "                    format='cds',\n",
    "                    readme=readme_path,\n",
    "                    guess=False,\n",
    "                    fast_reader=False,\n",
    "                    fill_values=[('-', 'nan'), ('--', 'nan'), ('...', 'nan')]\n",
    "                )\n",
    "                table_data = table_data.to_pandas()\n",
    "                table_data.to_csv(csv_path, index=False)\n",
    "                print(f\"✅ Converted table{n} to CSV.\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to read table{n}: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Missing table{n}.dat or ReadMe file for {system}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d0f7b",
   "metadata": {},
   "source": [
    "### HMS/DMS $\\leftrightarrow$ Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f17dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Coordinates:\n",
      "\n",
      "(16:10:32.30 , -08:15:38.50)\n",
      "(15:05:02.13 , -13:44:20.23)\n",
      "(18:09:43.61 , -40:52:07.99)\n"
     ]
    }
   ],
   "source": [
    "## Convert coordinates between degrees and hms/dms format\n",
    "\n",
    "data = '''\n",
    "242.63458901020093, -8.260693273003724 \n",
    "226.25888112673744, -13.738953843436517\n",
    "272.43169186461097, -40.86888561990257 \n",
    "'''\n",
    "\n",
    "def convert_deg_to_hmsdms(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_deg, dec_deg = map(float, line.split(','))\n",
    "        ra_hms = scoord.ra_deg_to_hms(ra_deg, precision=2)\n",
    "        dec_dms = scoord.dec_deg_to_dms(dec_deg, precision=2)\n",
    "        coords.append((ra_hms, dec_dms))\n",
    "        # print(f\"{ra_hms}, {dec_dms}\")\n",
    "    return coords\n",
    "\n",
    "def convert_hmsdms_to_deg(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_hms, dec_hms = map(str, line.split(','))\n",
    "        ra_deg = scoord.ra_hms_to_deg(ra_hms, precision=6)\n",
    "        dec_deg = scoord.dec_dms_to_deg(dec_hms, precision=6)\n",
    "        coords.append((ra_deg, dec_deg))\n",
    "        # print(f\"{ra_deg}, {dec_deg}\")\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Convert the coordinates\n",
    "try:\n",
    "    txt = convert_deg_to_hmsdms(data)\n",
    "except:\n",
    "    try:\n",
    "        txt = convert_hmsdms_to_deg(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting coordinates: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"Converted Coordinates:\\n\")\n",
    "for c in txt:\n",
    "    print(f'({c[0]} , {c[1]})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
