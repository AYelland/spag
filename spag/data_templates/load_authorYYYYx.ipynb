{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f6d567",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Comments & Instructions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f44282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a example/template for the `read_data.py` script, that loads the\n",
    "# data from a specific paper and reformats it into the standard format used by\n",
    "# SPAG. To use this, copy the following function into a testing script or\n",
    "# Jupyter notebook, and modify the function name and contents as needed.\n",
    "\n",
    "\n",
    "# Specifically, change all instances of...\n",
    "\n",
    "# - `authorYYYYx` to the first author's last name (maybe with their first \n",
    "# initial of there is a duplicate last name) and year of the paper. When there\n",
    "# are instances of an author releasing more than one paper in a year, append a\n",
    "# letter to the year in `x` position.\n",
    "#   (e.g., `Ji et al. 2016a` --> `ji2016a`, if there is also a \n",
    "#       `Ji et al. 2016b`)\n",
    "#   (e.g., `Hansen et al. 2018` --> `hansent2018c`, if there are multiple\n",
    "#       authors with the same last name and this was T. Hansen's 3rd paper in \n",
    "#       2018)\n",
    "\n",
    "# - the path to the data files to point to the correct location of the data \n",
    "#   tables.\n",
    "#   (e.g., `data_dir + \"abundance_tables/authorYYYYx/table1.csv\"`)\n",
    "\n",
    "# - The `Reference` and `Ref` columns to match the citation for the paper.\n",
    "#   --> `Reference` column should be in the form 'Author+Year'.\n",
    "#       (e.g., 'Ji+2016a', 'HansenT+2018c')\n",
    "#   --> `Ref` column should shortened form of the reference, typically the \n",
    "#       first 3 letters of the author's last name (all uppercase) and the last\n",
    "#       two digits of the year, with a letter appended if there are multiple\n",
    "#       papers by the same author(s) in that year. (e.g.'JI16a', 'HANt18c')\n",
    "\n",
    "# - The `Loc` column to match the type of object the stars are in. Use:\n",
    "#   --> 'HA' for halo stars\n",
    "#   --> 'BU' for bulge stars\n",
    "#   --> 'DS' for disk stars\n",
    "#   --> 'DW' for dwarf galaxy stars\n",
    "#   --> 'UF' for ultra-faint dwarf galaxy stars\n",
    "#   --> 'GC' for globular cluster stars\n",
    "\n",
    "# - The `System` column to match the name of the system the stars are in, if \n",
    "#   applicable and not already in the data table.\n",
    "\n",
    "\n",
    "# Beyond this, confirm that the datafiles are reading in correctly, and that\n",
    "# the columns are being filled in properly. You may need to modify how the data \n",
    "# is read in or the structure of the datafile themselves, depending on the \n",
    "# format of the data tables.\n",
    "\n",
    "# Things to be wary of...\n",
    "\n",
    "# - Some papers use different formats for the species names, such as \"Fe I\" vs\n",
    "#   \"FeI\" vs \"Fe1\". The `ion_to_species()` function should be able to handle \n",
    "#   most of these, but you may need to modify the datafile if there are any \n",
    "#   issues, or contact A. Yelland.\n",
    "\n",
    "# - Some papers may not provide all the necessary columns, such as separate\n",
    "#   upper/lower limit flag column. You may need to modify the datafile to add\n",
    "#   the flag columns.\n",
    "\n",
    "# - Additionally, different authors at different times may use different solar\n",
    "#   abundance scales. The `get_solar()` function can be used to retrieve the\n",
    "#   solar abundances of the most commonly used scales, but if one is not\n",
    "#   available, please let A. Yelland know.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153eca",
   "metadata": {},
   "source": [
    "---\n",
    "## Package Imports & Directories\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65303612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/j3qbky6s25108m4dm8y_pdfh0000gn/T/ipykernel_96522/4066198470.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.sgr]\n",
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.orphan]\n",
      "WARNING: AstropyDeprecationWarning: The matrix_product function is deprecated and may be removed in a future version.\n",
      "        Use @ instead. [gala.coordinates.magellanic_stream]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m sns_palette \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mcolor_palette()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# script_dir = \"/\".join(IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[:-1]) + \"/\" # use this if in ipython\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# use this if not in ipython (i.e. terminal script)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m script_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m plots_dir \u001b[38;5;241m=\u001b[39m script_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../plots/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import (division, print_function, absolute_import, unicode_literals)\n",
    "\n",
    "import pkg_resources\n",
    "import  sys, os, glob, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from astropy.io import fits\n",
    "\n",
    "from spag.read_data import *\n",
    "from spag.convert import *\n",
    "from spag.utils import *\n",
    "# from spag.calculate import *\n",
    "# import spag.read_data as rd\n",
    "# import spag.coordinates as coord\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns_palette = sns.color_palette()\n",
    "\n",
    "# script_dir = \"/\".join(IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[:-1]) + \"/\" # use this if in ipython\n",
    "script_dir = os.path.dirname(os.path.realpath(__file__))+\"/\" # use this if not in ipython (i.e. terminal script)\n",
    "data_dir = script_dir+\"../data/\"\n",
    "plots_dir = script_dir+\"../plots/\"\n",
    "linelist_dir = script_dir+\"../linelists/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb48c53",
   "metadata": {},
   "source": [
    "---\n",
    "## Template Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b595691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_authorYYYYx(io=None):\n",
    "    \"\"\"\n",
    "    Load the Author et al. YYYYx data for the <grouping or classification of stars>.\n",
    "\n",
    "    Table 1 - Observations\n",
    "    Table 2 - Stellar Parameters\n",
    "    Table 3 - Abundance Table\n",
    "    \"\"\"\n",
    "\n",
    "    ## Read in the data tables\n",
    "    obs_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table1.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    param_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table2.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "    abund_df = pd.read_csv(data_dir + \"abundance_tables/authorYYYYx/table3.csv\", comment=\"#\", na_values=[\"\", \" \", \"nan\", \"NaN\", \"N/A\", \"n/a\"])\n",
    "\n",
    "    ## Make the new column names\n",
    "    species = []\n",
    "    for ion in abund_df[\"Species\"].unique():\n",
    "        species_i = ion_to_species(ion)\n",
    "        elem_i = ion_to_element(ion)\n",
    "        if species_i not in species:\n",
    "            species.append(species_i)\n",
    "\n",
    "    epscols = [make_epscol(s) for s in species]\n",
    "    ulcols = [make_ulcol(s) for s in species]\n",
    "    XHcols = [make_XHcol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXHcols = ['ul' + col for col in XHcols]\n",
    "    XFecols = [make_XFecol(s).replace(\" \", \"\") for s in species]\n",
    "    ulXFecols = ['ul' + col for col in XFecols]\n",
    "    errcols = [make_errcol(s) for s in species]\n",
    "\n",
    "    ## New dataframe with proper columns\n",
    "    authorYYYYx_df = pd.DataFrame(\n",
    "                    columns=['I/O','Name','Simbad_Identifier','Reference','Ref','Loc','System','RA_hms','RA_deg','DEC_dms','DEC_deg',\n",
    "                    'Teff','logg','Fe/H','Vmic'] + epscols + ulcols + XHcols + ulXHcols + XFecols \n",
    "                    + ulXFecols + errcols)\n",
    "    for i, name in enumerate(abund_df['Name'].unique()):\n",
    "        authorYYYYx_df.loc[i,'Name'] = name\n",
    "        authorYYYYx_df.loc[i,'Simbad_Identifier'] = obs_df.loc[obs_df['Name'] == name, 'Simbad_Identifier'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Reference'] = 'Author+YYYYx'\n",
    "        authorYYYYx_df.loc[i,'Ref'] = 'AUTYYx'\n",
    "        authorYYYYx_df.loc[i,'I/O'] = 1\n",
    "        authorYYYYx_df.loc[i,'Loc'] = 'UF' # [HA, BU, DS, DW, UF, GC]\n",
    "        authorYYYYx_df.loc[i,'System'] = obs_df.loc[obs_df['Name'] == name, 'System'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_hms'] = obs_df.loc[obs_df['Name'] == name, 'RA_hms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'RA_deg'] = coord.ra_hms_to_deg(authorYYYYx_df.loc[i,'RA_hms'], precision=6)\n",
    "        authorYYYYx_df.loc[i,'DEC_dms'] = obs_df.loc[obs_df['Name'] == name, 'DEC_dms'].values[0]\n",
    "        authorYYYYx_df.loc[i,'DEC_deg'] = coord.dec_dms_to_deg(authorYYYYx_df.loc[i,'DEC_dms'], precision=2)\n",
    "        authorYYYYx_df.loc[i,'Teff'] = param_df.loc[param_df['Name'] == name, 'Teff'].values[0]\n",
    "        authorYYYYx_df.loc[i,'logg'] = param_df.loc[param_df['Name'] == name, 'logg'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Fe/H'] = param_df.loc[param_df['Name'] == name, 'Fe/H'].values[0]\n",
    "        authorYYYYx_df.loc[i,'Vmic'] = param_df.loc[param_df['Name'] == name, 'Vmic'].values[0]\n",
    "\n",
    "        ## Fill in data\n",
    "        star_df = abund_df[abund_df['Name'] == name]\n",
    "        for j, row in star_df.iterrows():\n",
    "            ion = row[\"Species\"]\n",
    "            species_i = ion_to_species(ion)\n",
    "            elem_i = ion_to_element(ion)\n",
    "\n",
    "            logepsX_sun_a09 = get_solar(elem_i, version='asplund2009')[0]\n",
    "            logepsFe_a09 = star_df.loc[star_df['Species'] == 'Fe I', 'logepsX'].values[0]\n",
    "            feh_a09 = logepsFe_a09 - get_solar('Fe', version='asplund2009')[0]\n",
    "\n",
    "            ## Assign epsX values\n",
    "            col = make_epscol(species_i)\n",
    "            if col in epscols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.isna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign ulX values\n",
    "            col = make_ulcol(species_i)\n",
    "            if col in ulcols:\n",
    "                authorYYYYx_df.loc[i, col] = row[\"logepsX\"] if pd.notna(row[\"l_logepsX\"]) else np.nan\n",
    "\n",
    "            ## Assign [X/H] and ul[X/H]values\n",
    "            col = make_XHcol(species_i).replace(\" \", \"\")\n",
    "            if col in XHcols:\n",
    "                if pd.isna(row[\"l_[X/H]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round(row[\"logepsX\"] - logepsX_sun_a09, 2)\n",
    "                if 'e_[X/H]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/H]\"]\n",
    "\n",
    "            ## Assign [X/Fe] values\n",
    "            col = make_XFecol(species_i).replace(\" \", \"\")\n",
    "            if col in XFecols:\n",
    "                if pd.isna(row[\"l_[X/Fe]\"]):\n",
    "                    authorYYYYx_df.loc[i, col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = np.nan\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "                    authorYYYYx_df.loc[i, 'ul'+col] = normal_round((row[\"logepsX\"] - logepsX_sun_a09) - feh_a09, 2)\n",
    "                if 'e_[X/Fe]' in row.index:\n",
    "                    authorYYYYx_df.loc[i, 'e_'+col] = row[\"e_[X/Fe]\"]\n",
    "\n",
    "            ## Assign error values\n",
    "            col = make_errcol(species_i)\n",
    "            if col in errcols:\n",
    "                e_logepsX = row.get(\"e_logepsX\", np.nan)\n",
    "                if pd.notna(e_logepsX):\n",
    "                    authorYYYYx_df.loc[i, col] = e_logepsX\n",
    "                else:\n",
    "                    authorYYYYx_df.loc[i, col] = np.nan\n",
    "\n",
    "    ## Drop the Fe/Fe columns\n",
    "    authorYYYYx_df.drop(columns=['[Fe/Fe]','ul[Fe/Fe]','[FeII/Fe]','ul[FeII/Fe]'], inplace=True, errors='ignore')\n",
    "\n",
    "    return authorYYYYx_df\n",
    "\n",
    "df = load_authorYYYYx()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceecde7",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d02d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can copy/paste the above function, and modify it for your specific reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20663730",
   "metadata": {},
   "source": [
    "---\n",
    "## Helpful Conversion Scripts for Different Datafiles & Numbers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ce3c2",
   "metadata": {},
   "source": [
    "### Abundance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd4e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logeps(Fe):  1.88\n",
      "logeps(Sr): <-3.90\n",
      "logeps(Ba):  -3.50\n",
      "[Fe/H]:      -5.62\n",
      "[Sr/H]:     <-6.77\n",
      "[Ba/H]:      -5.68\n",
      "[Sr/Fe]:    <-1.15\n",
      "[Ba/Fe]:     -0.06\n"
     ]
    }
   ],
   "source": [
    "## Quick Abundance Conversions\n",
    "\n",
    "### Solar Abundances\n",
    "epsfe_sun = rd.get_solar('Fe')[0]\n",
    "epsba_sun = rd.get_solar('Ba')[0]\n",
    "epssr_sun = rd.get_solar('Sr')[0]\n",
    "\n",
    "### From Anna's notes of a presentation\n",
    "srfe = -1.15\n",
    "epsba = -3.5\n",
    "feh = -5.62\n",
    "\n",
    "### Calculated log(eps) values\n",
    "srh = srfe + feh\n",
    "epssr = epssr_sun + srh\n",
    "epsfe = epsfe_sun + feh\n",
    "\n",
    "feh_new = epsfe - epsfe_sun\n",
    "srh_new = epssr - epssr_sun\n",
    "bah_new = epsba - epsba_sun\n",
    "\n",
    "srfe_new = srh_new - feh_new\n",
    "bafe_new = bah_new - feh_new\n",
    "\n",
    "print(f\"logeps(Fe):  {epsfe:.2f}\")\n",
    "print(f\"logeps(Sr): <{epssr:.2f}\")\n",
    "print(f\"logeps(Ba):  {epsba:.2f}\")\n",
    "print(f\"[Fe/H]:      {feh_new:.2f}\")\n",
    "print(f\"[Sr/H]:     <{srh_new:.2f}\")\n",
    "print(f\"[Ba/H]:      {bah_new:.2f}\")\n",
    "print(f\"[Sr/Fe]:    <{srfe_new:.2f}\")\n",
    "print(f\"[Ba/Fe]:     {bafe_new:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167a4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XH_from_eps(0.73, 'Sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac193c",
   "metadata": {},
   "source": [
    "### CDS $\\rightarrow$ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert all CDS Standard Formatted Tables to CSVs for Stellar Abundances\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system =  'francois2007'\n",
    "    table_numbers = [2,3,8]\n",
    "    for n in table_numbers:\n",
    "\n",
    "        base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data/abundance_tables/{system}/\"\n",
    "        table_path = base_path + f\"table{n}.dat\"\n",
    "        readme_path = base_path + f\"ReadMe\"\n",
    "        csv_path = base_path + f\"table{n}.csv\"\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            table_data = ascii.read(table_path, format='cds', readme=readme_path)\n",
    "                    # guess=False,\n",
    "                    # fast_reader=False\n",
    "                    \n",
    "            table_data = table_data.to_pandas()\n",
    "            table_data.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Table {table} or ReadMe file does not exist for {system}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted table1 to CSV.\n",
      "❌ Failed to read table2: Column V(MW) failed to convert: invalid literal for int() with base 10: 'nan'\n",
      "✅ Converted table3 to CSV.\n",
      "✅ Converted table4 to CSV.\n",
      "✅ Converted table5 to CSV.\n"
     ]
    }
   ],
   "source": [
    "## Convert CDS Standard Formatted Tables to CSVs for Galaxy Properties\n",
    "\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "\n",
    "create_csv_files = True\n",
    "if create_csv_files:\n",
    "    system = 'mcconnachie2012'\n",
    "    table_numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "    base_path = f\"/Users/ayelland/Research/metal-poor-stars/spag/spag/data/galaxy_properties/{system}/\"\n",
    "    readme_path = os.path.join(base_path, \"ReadMe\")\n",
    "\n",
    "    for n in table_numbers:\n",
    "        table_path = os.path.join(base_path, f\"table{n}.dat\")\n",
    "        csv_path = os.path.join(base_path, f\"table{n}.csv\")\n",
    "\n",
    "        if os.path.exists(table_path) and os.path.exists(readme_path):\n",
    "            try:\n",
    "                table_data = ascii.read(\n",
    "                    table_path,\n",
    "                    format='cds',\n",
    "                    readme=readme_path,\n",
    "                    guess=False,\n",
    "                    fast_reader=False,\n",
    "                    fill_values=[('-', 'nan'), ('--', 'nan'), ('...', 'nan')]\n",
    "                )\n",
    "                table_data = table_data.to_pandas()\n",
    "                table_data.to_csv(csv_path, index=False)\n",
    "                print(f\"✅ Converted table{n} to CSV.\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to read table{n}: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Missing table{n}.dat or ReadMe file for {system}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d0f7b",
   "metadata": {},
   "source": [
    "### HMS/DMS $\\leftrightarrow$ Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f17dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Coordinates:\n",
      "\n",
      "(16:10:32.30 , -08:15:38.50)\n",
      "(15:05:02.13 , -13:44:20.23)\n",
      "(18:09:43.61 , -40:52:07.99)\n"
     ]
    }
   ],
   "source": [
    "## Convert coordinates between degrees and hms/dms format\n",
    "\n",
    "data = '''\n",
    "242.63458901020093, -8.260693273003724 \n",
    "226.25888112673744, -13.738953843436517\n",
    "272.43169186461097, -40.86888561990257 \n",
    "'''\n",
    "\n",
    "def convert_deg_to_hmsdms(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_deg, dec_deg = map(float, line.split(','))\n",
    "        ra_hms = coord.ra_deg_to_hms(ra_deg, precision=2)\n",
    "        dec_dms = coord.dec_deg_to_dms(dec_deg, precision=2)\n",
    "        coords.append((ra_hms, dec_dms))\n",
    "        # print(f\"{ra_hms}, {dec_dms}\")\n",
    "    return coords\n",
    "\n",
    "def convert_hmsdms_to_deg(data):\n",
    "    \"\"\"\n",
    "    Convert the given data string of coordinates from degrees to hms/dms format.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        ra_hms, dec_hms = map(str, line.split(','))\n",
    "        ra_deg = coord.ra_hms_to_deg(ra_hms, precision=6)\n",
    "        dec_deg = coord.dec_dms_to_deg(dec_hms, precision=6)\n",
    "        coords.append((ra_deg, dec_deg))\n",
    "        # print(f\"{ra_deg}, {dec_deg}\")\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Convert the coordinates\n",
    "try:\n",
    "    txt = convert_deg_to_hmsdms(data)\n",
    "except:\n",
    "    try:\n",
    "        txt = convert_hmsdms_to_deg(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting coordinates: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"Converted Coordinates:\\n\")\n",
    "for c in txt:\n",
    "    print(f'({c[0]} , {c[1]})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
